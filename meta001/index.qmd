---
title: "Meta-Learning in Neural Networks: A Survey"
date: last-modified
categories: [Meta-Learning, Deep Learning, Survey]
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    theme: cosmo
    css: styles.css
---

# Meta-Learning in Neural Networks -- A Survey

## 1. ê°œìš” (Overview)

> **Paper Info**
>
> *   **Title:** Meta-Learning in Neural Networks: A Survey
> *   **Authors:** Hospedales et al.
> *   **Link:** [arXiv](https://arxiv.org/abs/2004.05439)

### ğŸ“Œ í•œ ì¤„ ìš”ì•½ (One-line Summary)
ê¸°ì¡´ ë”¥ëŸ¬ë‹ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´, ì—¬ëŸ¬ íƒœìŠ¤í¬ì˜ ê²½í—˜ì„ í†µí•´ "í•™ìŠµí•˜ëŠ” ë°©ë²•(How to learn)"ì„ í•™ìŠµí•˜ëŠ” ë©”íƒ€ëŸ¬ë‹ì˜ ìµœì‹  ë™í–¥ê³¼ ë¶„ë¥˜ ì²´ê³„ë¥¼ ì •ë¦¬í•œ ì„œë² ì´ ë…¼ë¬¸ì…ë‹ˆë‹¤.

### ğŸ¯ ë¬¸ì œ ì •ì˜ (Problem Statement)
*   **Conventional ML:** ê³ ì •ëœ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë§¤ë²ˆ ì²˜ìŒë¶€í„°(From Scratch) í•™ìŠµí•©ë‹ˆë‹¤. ì´ëŠ” ë°ì´í„°ì™€ ê³„ì‚° ìì›ì´ ë§ì´ í•„ìš”í•˜ë©°(Data/Compute Hungry), ì†Œìˆ˜ ë°ì´í„°(Few-shot) í™˜ê²½ì— ì·¨ì•½í•©ë‹ˆë‹¤.
*   **Goal:** ë©”íƒ€ëŸ¬ë‹ì€ ê³¼ê±°ì˜ ê²½í—˜(Experience)ì„ í™œìš©í•˜ì—¬ ìƒˆë¡œìš´ íƒœìŠ¤í¬ë¥¼ ë” íš¨ìœ¨ì ì´ê³  ë¹ ë¥´ê²Œ í•™ìŠµí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

### ğŸ”‘ ì£¼ìš” ê¸°ì—¬ (Contributions)
1.  **New Taxonomy:** Meta-Representation, Meta-Objective, Meta-Optimizerì— ê¸°ë°˜í•œ ìƒˆë¡œìš´ ë¶„ë¥˜ ì²´ê³„ ì œì‹œ.
2.  **Comprehensive Survey:** Few-shot, RL, NAS ë“± ë‹¤ì–‘í•œ ì‘ìš© ë¶„ì•¼ì™€ ìµœì‹  ë°©ë²•ë¡ ì„ í¬ê´„ì ìœ¼ë¡œ ì •ë¦¬.

---

## 2. ì£¼ìš” ê°œë… (Key Concepts)

### The Meta-Learning Setup
ê¸°ì¡´ í•™ìŠµ(Conventional Learning) vs ë©”íƒ€ í•™ìŠµ(Meta-Learning)ì˜ ì°¨ì´ì .

$$ \theta^* = \arg\min_\theta \mathcal{L}(\mathcal{D}; \theta) $$

*   **Task Distribution ($p(\mathcal{T})$):** ë©”íƒ€ëŸ¬ë‹ì€ ë‹¨ì¼ ë°ì´í„°ì…‹ì´ ì•„ë‹Œ íƒœìŠ¤í¬ë“¤ì˜ ë¶„í¬ì—ì„œ í•™ìŠµí•©ë‹ˆë‹¤.
*   **Support Set vs Query Set:** ê° íƒœìŠ¤í¬ëŠ” í•™ìŠµìš©(Support)ê³¼ í‰ê°€ìš©(Query) ë°ì´í„°ë¡œ ë‚˜ë‰©ë‹ˆë‹¤.
*   **Bilevel Optimization:**
    *   Inner Loop: íŠ¹ì • íƒœìŠ¤í¬ì— ì ì‘(Adaptation).
    *   Outer Loop: Inner Loopì˜ ê²°ê³¼ê°€ ì¢‹ë„ë¡ ë©”íƒ€ íŒŒë¼ë¯¸í„°($\omega$)ë¥¼ ìµœì í™”.

::: {.callout-note}
## Note
Meta-Learningì€ "Learning to Learn"ì´ë¼ê³ ë„ ë¶ˆë¦¬ë©°, ìƒˆë¡œìš´ íƒœìŠ¤í¬ì— ëŒ€í•´ ì ì€ ë°ì´í„°ë¡œ ë¹ ë¥´ê²Œ ì ì‘í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.
:::

---

## 3. ë°©ë²•ë¡  ë¶„ë¥˜ (Taxonomy of Methods)

ì´ ì„œë² ì´ ë…¼ë¬¸ì—ì„œëŠ” Meta-Learning ë°©ë²•ë¡ ì„ ì–´ë–»ê²Œ ë¶„ë¥˜í•˜ê³  ìˆëŠ”ê°€?

### 1) Metric-Based
*   Siamese Networks
*   Matching Networks
*   Prototypical Networks

### 2) Model-Based
*   MANN (Memory-Augmented Neural Networks)
*   Meta Networks

### 3) Optimization-Based
*   MAML (Model-Agnostic Meta-Learning)
*   Reptile

---

## 4. ì‹¤í—˜ ë° ê²°ê³¼ (Experiments & Benchmarks)

### Datasets
*   Omniglot
*   Mini-ImageNet

### Results
*ì£¼ìš” ë²¤ì¹˜ë§ˆí¬ì—ì„œì˜ ì„±ëŠ¥ ë¹„êµ*

---

## 5. ê²°ë¡  ë° ê³ ì°° (Conclusion & Thoughts)

### âœ… ì¥ì  (Pros)
*
*

### âš ï¸ í•œê³„ì  (Cons)
*
*

### ğŸ“ ê°œì¸ì ì¸ ìƒê° (Personal Notes)
*
